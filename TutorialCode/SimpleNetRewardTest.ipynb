{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b1d304-fd56-4936-93df-bd643840bc16",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d59811-56b0-4b45-be54-c592940871b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import Statements: From PyTorch RL Tutorial\n",
    "\"\"\"\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nle\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529c5c6-ced1-4680-b18f-4a80a9fc91c0",
   "metadata": {},
   "source": [
    "# Experiences & Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97e9f72-86c9-451d-a9e3-4ba130a58922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience():\n",
    "    def __init__(self, state, action, next_state, reward):\n",
    "        self.state - state\n",
    "        self.action = action\n",
    "        self.next_state = next_state\n",
    "        self.reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02cf1d4c-6dc4-4542-b702-8b1edea94080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, max_memory):\n",
    "        self.memory = []\n",
    "        self.max_memory = max_memory\n",
    "    \n",
    "    def push(self, newExp):\n",
    "        if len(self.memory) >= self.max_memory:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append(newExp)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee2f11-914d-4f8c-bd7d-d76a127645e4",
   "metadata": {},
   "source": [
    "# Policy (& Target) Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a54456f-2207-404e-be73-5df8e1896c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAlphaNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.model_stack = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(), # 21 * 79 * 16\n",
    "            nn.MaxPool2d(3, padding=(0,1)), # 7 * 27 * 16\n",
    "            nn.MaxPool2d(3, padding=(1,0)), # 3 * 9 * 16\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3*9*16, 3*64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3*64, 113),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "    def obs_to_x(self, obs):\n",
    "        x = np.stack((obs['glyphs'], state['chars'], state['colors'], state['specials']))\n",
    "        return torch.tensor(x, device=device)[None,:,:,:]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model_stack(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5330873d-cfd9-425d-a099-01a3507c1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLNetwork():\n",
    "    def __init__(self, Model, hyperparameters = {}):\n",
    "        # setup gpu if available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # set hyperparameters\n",
    "        self.set_hyperparameters(hyperparameters)\n",
    "        \n",
    "        # setup environment\n",
    "        self.env = gym.make('NetHackChallenge-v0', savedir=None)\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.memory = ReplayMemory(self.max_memory)\n",
    "        \n",
    "        # setup policy & target networks\n",
    "        self.policy_net = Model().to(self.device)\n",
    "        self.target_net = Model().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        \n",
    "        # tracking metrics through the games\n",
    "        self.steps_done = 0\n",
    "        self.episode_durations = []\n",
    "        self.episode_rewards = []\n",
    "        self.episode_avg_loss = []\n",
    "    \n",
    "    def set_hyperparameters(self, hyperparameters):\n",
    "        self.batch_size = hyperparameters.get('batch_size', 64)\n",
    "        self.gamma = hyperparameters.get('gamma', 0.999)\n",
    "        self.eps_start = hyperparameters.get('eps_start', 0.9)\n",
    "        self.eps_end = hyperparameters.get('eps_end', 0.05)\n",
    "        self.eps_decay = hyperparameters.get('eps_decay', 200)\n",
    "        self.target_update = hyperparameters.get('target_update', 10)\n",
    "        self.max_memory = hyperparameters.get('max_memory', 100000)\n",
    "        self.episodes = hyperparameters.get('episodes', 1)\n",
    "    \n",
    "    def select_action(state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # get the max policy value\n",
    "                # only is 1 at a time\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=self.device, dtype=torch.long)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
