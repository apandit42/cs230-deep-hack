{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8228d4b-0caf-491f-b412-02bb1d230f1a",
   "metadata": {},
   "source": [
    "# Improved IMPALA Algorithm with a Default Reward Function\n",
    "\n",
    "## Should make changes to some of the architecture, such as the number of hidden layers in the neural network, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6626387b-f339-4287-ba11-778d40c57661",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e89461cc-c169-4f92-9fbd-0ad43b99df09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "\n",
    "\"\"\"Will  tell you when there is an error\"\"\"\n",
    "# Necessary for multithreading.\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch import multiprocessing as mp\n",
    "    from torch import nn\n",
    "    from torch.nn import functional as F\n",
    "except ImportError:\n",
    "    logging.exception(\n",
    "        \"PyTorch not found. Please install the agent dependencies with \"\n",
    "        '`pip install \"nle[agent]\"`'\n",
    "    )\n",
    "\n",
    "    \n",
    "\"\"\"Import for nle. \"\"\"\n",
    "import gym  # noqa: E402\n",
    "import nle  # noqa: F401, E402\n",
    "from nle import nethack  # noqa: E402\n",
    "from nle.agent import vtrace  # noqa: E402"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9c2d92-4628-4287-bd79-4388f414ba21",
   "metadata": {},
   "source": [
    "# Parser for console run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7320385a-102a-4626-8c2c-8cd37aaff1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--grad_norm_clipping'], dest='grad_norm_clipping', nargs=None, const=None, default=40.0, type=<class 'float'>, choices=None, help='Global gradient norm clip.', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yapf: disable\n",
    "\"We aren't going to use these flags. Therefore, it is a little unessesary to have them. \"\n",
    "parser = argparse.ArgumentParser(description=\"PyTorch Scalable Agent\")\n",
    "\n",
    "parser.add_argument(\"--env\", type=str, default=\"NetHackScore-v0\",\n",
    "                    help=\"Gym environment.\")\n",
    "parser.add_argument(\"--mode\", default=\"train\",\n",
    "                    choices=[\"train\", \"test\", \"test_render\"],\n",
    "                    help=\"Training or test mode.\")\n",
    "\n",
    "# Training settings.\n",
    "parser.add_argument(\"--disable_checkpoint\", action=\"store_true\",\n",
    "                    help=\"Disable saving checkpoint.\")\n",
    "parser.add_argument(\"--savedir\", default=\"~/torchbeast/\",\n",
    "                    help=\"Root dir where experiment data will be saved.\")\n",
    "parser.add_argument(\"--num_actors\", default=4, type=int, metavar=\"N\",\n",
    "                    help=\"Number of actors (default: 4).\")\n",
    "parser.add_argument(\"--total_steps\", default=100000, type=int, metavar=\"T\",\n",
    "                    help=\"Total environment steps to train for.\")\n",
    "parser.add_argument(\"--batch_size\", default=8, type=int, metavar=\"B\",\n",
    "                    help=\"Learner batch size.\")\n",
    "parser.add_argument(\"--unroll_length\", default=80, type=int, metavar=\"T\",\n",
    "                    help=\"The unroll length (time dimension).\")\n",
    "parser.add_argument(\"--num_buffers\", default=None, type=int,\n",
    "                    metavar=\"N\", help=\"Number of shared-memory buffers.\")\n",
    "parser.add_argument(\"--num_learner_threads\", \"--num_threads\", default=2, type=int,\n",
    "                    metavar=\"N\", help=\"Number learner threads.\")\n",
    "parser.add_argument(\"--disable_cuda\", action=\"store_true\",\n",
    "                    help=\"Disable CUDA.\")\n",
    "parser.add_argument(\"--use_lstm\", action=\"store_true\",\n",
    "                    help=\"Use LSTM in agent model.\")\n",
    "\n",
    "# Loss settings.\n",
    "parser.add_argument(\"--entropy_cost\", default=0.0006,\n",
    "                    type=float, help=\"Entropy cost/multiplier.\")\n",
    "parser.add_argument(\"--baseline_cost\", default=0.5,\n",
    "                    type=float, help=\"Baseline cost/multiplier.\")\n",
    "parser.add_argument(\"--discounting\", default=0.99,\n",
    "                    type=float, help=\"Discounting factor.\")\n",
    "parser.add_argument(\"--reward_clipping\", default=\"abs_one\",\n",
    "                    choices=[\"abs_one\", \"none\"],\n",
    "                    help=\"Reward clipping.\")\n",
    "\n",
    "# Optimizer settings.\n",
    "parser.add_argument(\"--learning_rate\", default=0.00048,\n",
    "                    type=float, metavar=\"LR\", help=\"Learning rate.\")\n",
    "parser.add_argument(\"--alpha\", default=0.99, type=float,\n",
    "                    help=\"RMSProp smoothing constant.\")\n",
    "parser.add_argument(\"--momentum\", default=0, type=float,\n",
    "                    help=\"RMSProp momentum.\")\n",
    "parser.add_argument(\"--epsilon\", default=0.01, type=float,\n",
    "                    help=\"RMSProp epsilon.\")\n",
    "parser.add_argument(\"--grad_norm_clipping\", default=40.0, type=float,\n",
    "                    help=\"Global gradient norm clip.\")\n",
    "# yapf: enable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "680a3308-ed7b-48dc-8cfb-0b6616bb077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=(\n",
    "        \"[%(levelname)s:%(process)d %(module)s:%(lineno)d %(asctime)s] \" \"%(message)s\"\n",
    "    ),\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0a70aa-b67d-4d2c-a074-b7a3383594d9",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4fa85f-76fd-42c6-9eb9-05334a863088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_map(f, n):\n",
    "    if isinstance(n, tuple) or isinstance(n, list):\n",
    "        return n.__class__(nested_map(f, sn) for sn in n)\n",
    "    elif isinstance(n, dict):\n",
    "        return {k: nested_map(f, v) for k, v in n.items()}\n",
    "    else:\n",
    "        return f(n)\n",
    "\n",
    "\n",
    "def compute_baseline_loss(advantages):\n",
    "    return 0.5 * torch.sum(advantages ** 2)\n",
    "\n",
    "\n",
    "def compute_entropy_loss(logits):\n",
    "    \"\"\"Return the entropy loss, i.e., the negative entropy of the policy.\"\"\"\n",
    "    policy = F.softmax(logits, dim=-1)\n",
    "    log_policy = F.log_softmax(logits, dim=-1)\n",
    "    return torch.sum(policy * log_policy)\n",
    "\n",
    "\n",
    "def compute_policy_gradient_loss(logits, actions, advantages):\n",
    "    cross_entropy = F.nll_loss(\n",
    "        F.log_softmax(torch.flatten(logits, 0, 1), dim=-1),\n",
    "        target=torch.flatten(actions, 0, 1),\n",
    "        reduction=\"none\",\n",
    "    )\n",
    "    cross_entropy = cross_entropy.view_as(advantages)\n",
    "    return torch.sum(cross_entropy * advantages.detach())\n",
    "\n",
    "\n",
    "def create_env(name, *args, **kwargs):\n",
    "    return gym.make(name, observation_keys=(\"glyphs\", \"blstats\"), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a43dc4-1597-44c5-8101-3041e5fccd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "    full_queue: mp.SimpleQueue,\n",
    "    model: torch.nn.Module,\n",
    "    buffers,\n",
    "    initial_agent_state_buffers,\n",
    "):\n",
    "    try:\n",
    "        logging.info(\"Actor %i started.\", actor_index)\n",
    "\n",
    "        gym_env = create_env(flags.env, savedir=flags.rundir)\n",
    "        env = ResettingEnvironment(gym_env)\n",
    "        env_output = env.initial()\n",
    "        agent_state = model.initial_state(batch_size=1)\n",
    "        agent_output, unused_state = model(env_output, agent_state)\n",
    "        while True:\n",
    "            index = free_queue.get()\n",
    "            if index is None:\n",
    "                break\n",
    "\n",
    "            # Write old rollout end.\n",
    "            for key in env_output:\n",
    "                buffers[key][index][0, ...] = env_output[key]\n",
    "            for key in agent_output:\n",
    "                buffers[key][index][0, ...] = agent_output[key]\n",
    "            for i, tensor in enumerate(agent_state):\n",
    "                initial_agent_state_buffers[index][i][...] = tensor\n",
    "\n",
    "            # Do new rollout.\n",
    "            for t in range(flags.unroll_length):\n",
    "                with torch.no_grad():\n",
    "                    agent_output, agent_state = model(env_output, agent_state)\n",
    "\n",
    "                env_output = env.step(agent_output[\"action\"])\n",
    "\n",
    "                for key in env_output:\n",
    "                    buffers[key][index][t + 1, ...] = env_output[key]\n",
    "                for key in agent_output:\n",
    "                    buffers[key][index][t + 1, ...] = agent_output[key]\n",
    "\n",
    "            full_queue.put(index)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        print()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53de7f1e-0624-43d3-88d6-8950febebca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(\n",
    "    flags,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "    full_queue: mp.SimpleQueue,\n",
    "    buffers,\n",
    "    initial_agent_state_buffers,\n",
    "    lock=threading.Lock(),\n",
    "):\n",
    "    with lock:\n",
    "        indices = [full_queue.get() for _ in range(flags.batch_size)]\n",
    "    batch = {\n",
    "        key: torch.stack([buffers[key][m] for m in indices], dim=1) for key in buffers\n",
    "    }\n",
    "    initial_agent_state = (\n",
    "        torch.cat(ts, dim=1)\n",
    "        for ts in zip(*[initial_agent_state_buffers[m] for m in indices])\n",
    "    )\n",
    "    for m in indices:\n",
    "        free_queue.put(m)\n",
    "    batch = {k: t.to(device=flags.device, non_blocking=True) for k, t in batch.items()}\n",
    "    initial_agent_state = tuple(\n",
    "        t.to(device=flags.device, non_blocking=True) for t in initial_agent_state\n",
    "    )\n",
    "    return batch, initial_agent_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e833be41-1dfa-4115-9edd-fbc700410356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(\n",
    "    flags,\n",
    "    actor_model,\n",
    "    model,\n",
    "    batch,\n",
    "    initial_agent_state,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    lock=threading.Lock(),  # noqa: B008\n",
    "):\n",
    "    \"\"\"Performs a learning (optimization) step.\"\"\"\n",
    "    with lock:\n",
    "        learner_outputs, unused_state = model(batch, initial_agent_state)\n",
    "\n",
    "        # Take final value function slice for bootstrapping.\n",
    "        bootstrap_value = learner_outputs[\"baseline\"][-1]\n",
    "\n",
    "        # Move from obs[t] -> action[t] to action[t] -> obs[t].\n",
    "        batch = {key: tensor[1:] for key, tensor in batch.items()}\n",
    "        learner_outputs = {key: tensor[:-1] for key, tensor in learner_outputs.items()}\n",
    "\n",
    "        rewards = batch[\"reward\"]\n",
    "        if flags.reward_clipping == \"abs_one\":\n",
    "            clipped_rewards = torch.clamp(rewards, -1, 1)\n",
    "        elif flags.reward_clipping == \"none\":\n",
    "            clipped_rewards = rewards\n",
    "\n",
    "        discounts = (~batch[\"done\"]).float() * flags.discounting\n",
    "\n",
    "        vtrace_returns = vtrace.from_logits(\n",
    "            behavior_policy_logits=batch[\"policy_logits\"],\n",
    "            target_policy_logits=learner_outputs[\"policy_logits\"],\n",
    "            actions=batch[\"action\"],\n",
    "            discounts=discounts,\n",
    "            rewards=clipped_rewards,\n",
    "            values=learner_outputs[\"baseline\"],\n",
    "            bootstrap_value=bootstrap_value,\n",
    "        )\n",
    "\n",
    "        pg_loss = compute_policy_gradient_loss(\n",
    "            learner_outputs[\"policy_logits\"],\n",
    "            batch[\"action\"],\n",
    "            vtrace_returns.pg_advantages,\n",
    "        )\n",
    "        baseline_loss = flags.baseline_cost * compute_baseline_loss(\n",
    "            vtrace_returns.vs - learner_outputs[\"baseline\"]\n",
    "        )\n",
    "        entropy_loss = flags.entropy_cost * compute_entropy_loss(\n",
    "            learner_outputs[\"policy_logits\"]\n",
    "        )\n",
    "\n",
    "        total_loss = pg_loss + baseline_loss + entropy_loss\n",
    "\n",
    "        episode_returns = batch[\"episode_return\"][batch[\"done\"]]\n",
    "        stats = {\n",
    "            \"episode_returns\": tuple(episode_returns.cpu().numpy()),\n",
    "            \"mean_episode_return\": torch.mean(episode_returns).item(),\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"pg_loss\": pg_loss.item(),\n",
    "            \"baseline_loss\": baseline_loss.item(),\n",
    "            \"entropy_loss\": entropy_loss.item(),\n",
    "        }\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), flags.grad_norm_clipping)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        actor_model.load_state_dict(model.state_dict())\n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e09b1197-bd41-4662-ab96-9983a89ae448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_buffers(flags, observation_space, num_actions, num_overlapping_steps=1):\n",
    "    size = (flags.unroll_length + num_overlapping_steps,)\n",
    "\n",
    "    # Get specimens to infer shapes and dtypes.\n",
    "    samples = {k: torch.from_numpy(v) for k, v in observation_space.sample().items()}\n",
    "\n",
    "    specs = {\n",
    "        key: dict(size=size + sample.shape, dtype=sample.dtype)\n",
    "        for key, sample in samples.items()\n",
    "    }\n",
    "    specs.update(\n",
    "        reward=dict(size=size, dtype=torch.float32),\n",
    "        done=dict(size=size, dtype=torch.bool),\n",
    "        episode_return=dict(size=size, dtype=torch.float32),\n",
    "        episode_step=dict(size=size, dtype=torch.int32),\n",
    "        policy_logits=dict(size=size + (num_actions,), dtype=torch.float32),\n",
    "        baseline=dict(size=size, dtype=torch.float32),\n",
    "        last_action=dict(size=size, dtype=torch.int64),\n",
    "        action=dict(size=size, dtype=torch.int64),\n",
    "    )\n",
    "    buffers = {key: [] for key in specs}\n",
    "    for _ in range(flags.num_buffers):\n",
    "        for key in buffers:\n",
    "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
    "    return buffers\n",
    "\n",
    "\n",
    "def _format_observations(observation, keys=(\"glyphs\", \"blstats\")):\n",
    "    observations = {}\n",
    "    for key in keys:\n",
    "        entry = observation[key]\n",
    "        entry = torch.from_numpy(entry)\n",
    "        entry = entry.view((1, 1) + entry.shape)  # (...) -> (T,B,...).\n",
    "        observations[key] = entry\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041bd65c-34ee-45b0-88c9-5f95b18154a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4056ab0-bf5e-4bf9-8d38-5e44a85740e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResettingEnvironment:\n",
    "    \"\"\"Turns a Gym environment into something that can be step()ed indefinitely.\"\"\"\n",
    "\n",
    "    def __init__(self, gym_env):\n",
    "        self.gym_env = gym_env\n",
    "        self.episode_return = None\n",
    "        self.episode_step = None\n",
    "\n",
    "    def initial(self):\n",
    "        initial_reward = torch.zeros(1, 1)\n",
    "        # This supports only single-tensor actions ATM.\n",
    "        initial_last_action = torch.zeros(1, 1, dtype=torch.int64)\n",
    "        self.episode_return = torch.zeros(1, 1)\n",
    "        self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n",
    "        initial_done = torch.ones(1, 1, dtype=torch.uint8)\n",
    "\n",
    "        result = _format_observations(self.gym_env.reset())\n",
    "        result.update(\n",
    "            reward=initial_reward,\n",
    "            done=initial_done,\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            last_action=initial_last_action,\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, unused_info = self.gym_env.step(action.item())\n",
    "        self.episode_step += 1\n",
    "        self.episode_return += reward\n",
    "        episode_step = self.episode_step\n",
    "        episode_return = self.episode_return\n",
    "        if done:\n",
    "            observation = self.gym_env.reset()\n",
    "            self.episode_return = torch.zeros(1, 1)\n",
    "            self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n",
    "\n",
    "        result = _format_observations(observation)\n",
    "\n",
    "        reward = torch.tensor(reward).view(1, 1)\n",
    "        done = torch.tensor(done).view(1, 1)\n",
    "\n",
    "        result.update(\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "            episode_return=episode_return,\n",
    "            episode_step=episode_step,\n",
    "            last_action=action,\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def close(self):\n",
    "        self.gym_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900088c-f46d-4b7f-b6b4-5a6bf7071f28",
   "metadata": {},
   "source": [
    "# Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dea29a13-efc3-43a9-8db6-f385c4c44c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(flags):  # pylint: disable=too-many-branches, too-many-statements\n",
    "    flags.savedir = os.path.expandvars(os.path.expanduser(flags.savedir))\n",
    "\n",
    "    rundir = os.path.join(\n",
    "        flags.savedir, \"torchbeast-%s\" % time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(rundir):\n",
    "        os.makedirs(rundir)\n",
    "    logging.info(\"Logging results to %s\", rundir)\n",
    "\n",
    "    symlink = os.path.join(flags.savedir, \"latest\")\n",
    "    try:\n",
    "        if os.path.islink(symlink):\n",
    "            os.remove(symlink)\n",
    "        if not os.path.exists(symlink):\n",
    "            os.symlink(rundir, symlink)\n",
    "        logging.info(\"Symlinked log directory: %s\", symlink)\n",
    "    except OSError:\n",
    "        raise\n",
    "\n",
    "    logfile = open(os.path.join(rundir, \"logs.tsv\"), \"a\", buffering=1)\n",
    "    checkpointpath = os.path.join(rundir, \"model.tar\")\n",
    "\n",
    "    flags.rundir = rundir\n",
    "\n",
    "    if flags.num_buffers is None:  # Set sensible default for num_buffers.\n",
    "        flags.num_buffers = max(2 * flags.num_actors, flags.batch_size)\n",
    "    if flags.num_actors >= flags.num_buffers:\n",
    "        raise ValueError(\"num_buffers should be larger than num_actors\")\n",
    "    if flags.num_buffers < flags.batch_size:\n",
    "        raise ValueError(\"num_buffers should be larger than batch_size\")\n",
    "\n",
    "    T = flags.unroll_length\n",
    "    B = flags.batch_size\n",
    "\n",
    "    flags.device = None\n",
    "    if not flags.disable_cuda and torch.cuda.is_available():\n",
    "        logging.info(\"Using CUDA.\")\n",
    "        flags.device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        logging.info(\"Not using CUDA.\")\n",
    "        flags.device = torch.device(\"cpu\")\n",
    "\n",
    "    env = create_env(flags.env)\n",
    "    observation_space = env.observation_space\n",
    "    action_space = env.action_space\n",
    "    del env  # End this before forking.\n",
    "\n",
    "    model = Net(observation_space, action_space.n, flags.use_lstm)\n",
    "    buffers = create_buffers(flags, observation_space, model.num_actions)\n",
    "\n",
    "    model.share_memory()\n",
    "\n",
    "    # Add initial RNN state.\n",
    "    initial_agent_state_buffers = []\n",
    "    for _ in range(flags.num_buffers):\n",
    "        state = model.initial_state(batch_size=1)\n",
    "        for t in state:\n",
    "            t.share_memory_()\n",
    "        initial_agent_state_buffers.append(state)\n",
    "\n",
    "    actor_processes = []\n",
    "    ctx = mp.get_context(\"fork\")\n",
    "    free_queue = ctx.SimpleQueue()\n",
    "    full_queue = ctx.SimpleQueue()\n",
    "\n",
    "    for i in range(flags.num_actors):\n",
    "        actor = ctx.Process(\n",
    "            target=act,\n",
    "            args=(\n",
    "                flags,\n",
    "                i,\n",
    "                free_queue,\n",
    "                full_queue,\n",
    "                model,\n",
    "                buffers,\n",
    "                initial_agent_state_buffers,\n",
    "            ),\n",
    "            name=\"Actor-%i\" % i,\n",
    "        )\n",
    "        actor.start()\n",
    "        actor_processes.append(actor)\n",
    "\n",
    "    learner_model = Net(observation_space, action_space.n, flags.use_lstm).to(\n",
    "        device=flags.device\n",
    "    )\n",
    "    learner_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        learner_model.parameters(),\n",
    "        lr=flags.learning_rate,\n",
    "        momentum=flags.momentum,\n",
    "        eps=flags.epsilon,\n",
    "        alpha=flags.alpha,\n",
    "    )\n",
    "\n",
    "    def lr_lambda(epoch):\n",
    "        return 1 - min(epoch * T * B, flags.total_steps) / flags.total_steps\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    stat_keys = [\n",
    "        \"total_loss\",\n",
    "        \"mean_episode_return\",\n",
    "        \"pg_loss\",\n",
    "        \"baseline_loss\",\n",
    "        \"entropy_loss\",\n",
    "    ]\n",
    "    logfile.write(\"# Step\\t%s\\n\" % \"\\t\".join(stat_keys))\n",
    "\n",
    "    step, stats = 0, {}\n",
    "\n",
    "    def batch_and_learn(i, lock=threading.Lock()):\n",
    "        \"\"\"Thread target for the learning process.\"\"\"\n",
    "        nonlocal step, stats\n",
    "        while step < flags.total_steps:\n",
    "            batch, agent_state = get_batch(\n",
    "                flags, free_queue, full_queue, buffers, initial_agent_state_buffers\n",
    "            )\n",
    "            stats = learn(\n",
    "                flags, model, learner_model, batch, agent_state, optimizer, scheduler\n",
    "            )\n",
    "            with lock:\n",
    "                logfile.write(\"%i\\t\" % step)\n",
    "                logfile.write(\"\\t\".join(str(stats[k]) for k in stat_keys))\n",
    "                logfile.write(\"\\n\")\n",
    "                step += T * B\n",
    "\n",
    "    for m in range(flags.num_buffers):\n",
    "        free_queue.put(m)\n",
    "\n",
    "    threads = []\n",
    "    for i in range(flags.num_learner_threads):\n",
    "        thread = threading.Thread(\n",
    "            target=batch_and_learn,\n",
    "            name=\"batch-and-learn-%d\" % i,\n",
    "            args=(i,),\n",
    "            daemon=True,  # To support KeyboardInterrupt below.\n",
    "        )\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    def checkpoint():\n",
    "        if flags.disable_checkpoint:\n",
    "            return\n",
    "        logging.info(\"Saving checkpoint to %s\", checkpointpath)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"flags\": vars(flags),\n",
    "            },\n",
    "            checkpointpath,\n",
    "        )\n",
    "\n",
    "    timer = timeit.default_timer\n",
    "    try:\n",
    "        last_checkpoint_time = timer()\n",
    "        while step < flags.total_steps:\n",
    "            start_step = step\n",
    "            start_time = timer()\n",
    "            time.sleep(5)\n",
    "\n",
    "            if timer() - last_checkpoint_time > 10 * 60:  # Save every 10 min.\n",
    "                checkpoint()\n",
    "                last_checkpoint_time = timer()\n",
    "\n",
    "            sps = (step - start_step) / (timer() - start_time)\n",
    "            if stats.get(\"episode_returns\", None):\n",
    "                mean_return = (\n",
    "                    \"Return per episode: %.1f. \" % stats[\"mean_episode_return\"]\n",
    "                )\n",
    "            else:\n",
    "                mean_return = \"\"\n",
    "            total_loss = stats.get(\"total_loss\", float(\"inf\"))\n",
    "            logging.info(\n",
    "                \"Steps %i @ %.1f SPS. Loss %f. %sStats:\\n%s\",\n",
    "                step,\n",
    "                sps,\n",
    "                total_loss,\n",
    "                mean_return,\n",
    "                pprint.pformat(stats),\n",
    "            )\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"Quitting.\")\n",
    "        return  # Try joining actors then quit.\n",
    "    else:\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        logging.info(\"Learning finished after %d steps.\", step)\n",
    "    finally:\n",
    "        for _ in range(flags.num_actors):\n",
    "            free_queue.put(None)\n",
    "        for actor in actor_processes:\n",
    "            actor.join(timeout=1)\n",
    "\n",
    "    checkpoint()\n",
    "    logfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759ccdd-c6b4-429b-a31f-0a4fede49e5f",
   "metadata": {},
   "source": [
    "# Testing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efc87ee-4110-41a4-9fda-1c2bfcb01682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function: test\n",
    "\n",
    "This function takes care of testing. Takes care of seeds, runs the environment.\n",
    "\"\"\"\n",
    "def test(flags, num_episodes=10):\n",
    "    flags.savedir = os.path.expandvars(os.path.expanduser(flags.savedir))\n",
    "    checkpointpath = os.path.join(flags.savedir, \"latest\", \"model.tar\")\n",
    "\n",
    "    \"\"\"Creates the gym environment, resets it, and starts the NN classificatio\"\"\"\n",
    "    gym_env = create_env(flags.env)\n",
    "    env = ResettingEnvironment(gym_env)\n",
    "    model = Net(gym_env.observation_space, gym_env.action_space.n, flags.use_lstm)\n",
    "    model.eval()\n",
    "    \"\"\"\n",
    "    If there is a checkpoint available, then load the model \n",
    "    back up to continue to train it. \n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(checkpointpath, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    observation = env.initial()\n",
    "    returns = []\n",
    "\n",
    "    agent_state = model.initial_state(batch_size=1)\n",
    "\n",
    "    while len(returns) < num_episodes:\n",
    "        if flags.mode == \"test_render\":\n",
    "            env.gym_env.render()\n",
    "        policy_outputs, agent_state = model(observation, agent_state)\n",
    "        observation = env.step(policy_outputs[\"action\"])\n",
    "        if observation[\"done\"].item():\n",
    "            returns.append(observation[\"episode_return\"].item())\n",
    "            logging.info(\n",
    "                \"Episode ended after %d steps. Return: %.1f\",\n",
    "                observation[\"episode_step\"].item(),\n",
    "                observation[\"episode_return\"].item(),\n",
    "            )\n",
    "    env.close()\n",
    "    logging.info(\n",
    "        \"Average returns over %i steps: %.1f\", num_episodes, sum(returns) / len(returns)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480dc7b-0e7e-4079-a2e0-d48bfe192f3b",
   "metadata": {},
   "source": [
    "# Random Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cc68a00-1186-4119-96fc-c046fb6250d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Random class for a random agent. \n",
    "\"\"\"\n",
    "class RandomNet(nn.Module):\n",
    "    def __init__(self, observation_shape, num_actions, use_lstm):\n",
    "        super(RandomNet, self).__init__()\n",
    "        del observation_shape, use_lstm\n",
    "        self.num_actions = num_actions\n",
    "        self.theta = torch.nn.Parameter(torch.zeros(self.num_actions))\n",
    "\n",
    "    def forward(self, inputs, core_state):\n",
    "        # print(inputs)\n",
    "        T, B, *_ = inputs[\"observation\"].shape\n",
    "        zeros = self.theta * 0\n",
    "        # set logits to 0\n",
    "        policy_logits = zeros[None, :].expand(T * B, -1)\n",
    "        # set baseline to 0\n",
    "        baseline = policy_logits.sum(dim=1).view(-1, B)\n",
    "        \n",
    "        # Sample random action\n",
    "        action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1).view(\n",
    "            T, B\n",
    "        )\n",
    "        \n",
    "        \"\"\"Get the probabilities for each action.\"\"\"\n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        return (\n",
    "            dict(policy_logits=policy_logits, baseline=baseline, action=action),\n",
    "            core_state,\n",
    "        )\n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        return ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "611cc914-c616-4ad1-8167-70648d52ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step_to_range(delta, num_steps):\n",
    "    \"\"\"Range of `num_steps` integers with distance `delta` centered around zero.\"\"\"\n",
    "    return delta * torch.arange(-num_steps // 2, num_steps // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b7bdbde-41b2-4c21-9f54-5a7bb131639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Crop helper function for NetHackNet function\n",
    "\"\"\"\n",
    "class Crop(nn.Module):\n",
    "    \"\"\"Helper class for NetHackNet below.\"\"\"\n",
    "\n",
    "    def __init__(self, height, width, height_target, width_target):\n",
    "        super(Crop, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.width_target = width_target\n",
    "        self.height_target = height_target\n",
    "        width_grid = _step_to_range(2 / (self.width - 1), self.width_target)[\n",
    "            None, :\n",
    "        ].expand(self.height_target, -1)\n",
    "        height_grid = _step_to_range(2 / (self.height - 1), height_target)[\n",
    "            :, None\n",
    "        ].expand(-1, self.width_target)\n",
    "\n",
    "        # \"clone\" necessary, https://github.com/pytorch/pytorch/issues/34880\n",
    "        self.register_buffer(\"width_grid\", width_grid.clone())\n",
    "        self.register_buffer(\"height_grid\", height_grid.clone())\n",
    "\n",
    "    def forward(self, inputs, coordinates):\n",
    "        \"\"\"Calculates centered crop around given x,y coordinates.\n",
    "        Args:\n",
    "           inputs [B x H x W]\n",
    "           coordinates [B x 2] x,y coordinates\n",
    "        Returns:\n",
    "           [B x H' x W'] inputs cropped and centered around x,y coordinates.\n",
    "        \"\"\"\n",
    "        assert inputs.shape[1] == self.height\n",
    "        assert inputs.shape[2] == self.width\n",
    "\n",
    "        inputs = inputs[:, None, :, :].float()\n",
    "\n",
    "        x = coordinates[:, 0]\n",
    "        y = coordinates[:, 1]\n",
    "\n",
    "        x_shift = 2 / (self.width - 1) * (x.float() - self.width // 2)\n",
    "        y_shift = 2 / (self.height - 1) * (y.float() - self.height // 2)\n",
    "\n",
    "        grid = torch.stack(\n",
    "            [\n",
    "                self.width_grid[None, :, :] + x_shift[:, None, None],\n",
    "                self.height_grid[None, :, :] + y_shift[:, None, None],\n",
    "            ],\n",
    "            dim=3,\n",
    "        )\n",
    "\n",
    "        # TODO: only cast to int if original tensor was int\n",
    "        return (\n",
    "            torch.round(F.grid_sample(inputs, grid, align_corners=True))\n",
    "            .squeeze(1)\n",
    "            .long()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6374b2-680c-4f0d-959c-409efffc7a41",
   "metadata": {},
   "source": [
    "# The NetHack Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ef0aa6a-3d87-44e2-850d-79d8ddd7e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class: NetHackNet\n",
    "\n",
    "Params: nn.Module -> Used to create PyTorch NN our environment. \n",
    "\"\"\" \n",
    "class NetHackNet(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_shape,\n",
    "        num_actions,\n",
    "        use_lstm,\n",
    "        embedding_dim=32,\n",
    "        crop_dim=9,\n",
    "        num_layers=5,\n",
    "    ):\n",
    "        super(NetHackNet, self).__init__()\n",
    "\n",
    "        self.glyph_shape = observation_shape[\"glyphs\"].shape\n",
    "        self.blstats_size = observation_shape[\"blstats\"].shape[0]\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.use_lstm = use_lstm\n",
    "\n",
    "        self.H = self.glyph_shape[0]\n",
    "        self.W = self.glyph_shape[1]\n",
    "\n",
    "        self.k_dim = embedding_dim\n",
    "        self.h_dim = 512\n",
    "\n",
    "        self.crop_dim = crop_dim\n",
    "\n",
    "        self.crop = Crop(self.H, self.W, self.crop_dim, self.crop_dim)\n",
    "\n",
    "        self.embed = nn.Embedding(nethack.MAX_GLYPH, self.k_dim)\n",
    "\n",
    "        K = embedding_dim  # number of input filters\n",
    "        F = 3  # filter dimensions\n",
    "        S = 1  # stride\n",
    "        P = 1  # padding\n",
    "        M = 16  # number of intermediate filters\n",
    "        Y = 8  # number of output filters\n",
    "        L = num_layers  # number of convnet layers\n",
    "\n",
    "        in_channels = [K] + [M] * (L - 1)\n",
    "        out_channels = [M] * (L - 1) + [Y]\n",
    "\n",
    "        def interleave(xs, ys):\n",
    "            return [val for pair in zip(xs, ys) for val in pair]\n",
    "\n",
    "        conv_extract = [\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels[i],\n",
    "                out_channels=out_channels[i],\n",
    "                kernel_size=(F, F),\n",
    "                stride=S,\n",
    "                padding=P,\n",
    "            )\n",
    "            for i in range(L)\n",
    "        ]\n",
    "\n",
    "        self.extract_representation = nn.Sequential(\n",
    "            *interleave(conv_extract, [nn.ELU()] * len(conv_extract))\n",
    "        )\n",
    "\n",
    "        # CNN crop model.\n",
    "        conv_extract_crop = [\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels[i],\n",
    "                out_channels=out_channels[i],\n",
    "                kernel_size=(F, F),\n",
    "                stride=S,\n",
    "                padding=P,\n",
    "            )\n",
    "            for i in range(L)\n",
    "        ]\n",
    "\n",
    "        self.extract_crop_representation = nn.Sequential(\n",
    "            *interleave(conv_extract_crop, [nn.ELU()] * len(conv_extract))\n",
    "        )\n",
    "\n",
    "        out_dim = self.k_dim\n",
    "        # CNN over full glyph map\n",
    "        out_dim += self.H * self.W * Y\n",
    "\n",
    "        # CNN crop model.\n",
    "        out_dim += self.crop_dim ** 2 * Y\n",
    "\n",
    "        self.embed_blstats = nn.Sequential(\n",
    "            nn.Linear(self.blstats_size, self.k_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.k_dim, self.k_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(out_dim, self.h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.h_dim, self.h_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        if self.use_lstm:\n",
    "            self.core = nn.LSTM(self.h_dim, self.h_dim, num_layers=1)\n",
    "\n",
    "        self.policy = nn.Linear(self.h_dim, self.num_actions)\n",
    "        self.baseline = nn.Linear(self.h_dim, 1)\n",
    "\n",
    "    def initial_state(self, batch_size=1):\n",
    "        if not self.use_lstm:\n",
    "            return tuple()\n",
    "        return tuple(\n",
    "            torch.zeros(self.core.num_layers, batch_size, self.core.hidden_size)\n",
    "            for _ in range(2)\n",
    "        )\n",
    "\n",
    "    def _select(self, embed, x):\n",
    "        # Work around slow backward pass of nn.Embedding, see\n",
    "        # https://github.com/pytorch/pytorch/issues/24912\n",
    "        out = embed.weight.index_select(0, x.reshape(-1))\n",
    "        return out.reshape(x.shape + (-1,))\n",
    "    \n",
    "    \"\"\"\n",
    "    Class - NetHackNet\n",
    "    \n",
    "    Function: forward -> Performs a forward pass for our NN\n",
    "    \"\"\"\n",
    "    def forward(self, env_outputs, core_state):\n",
    "        \"\"\"\n",
    "        The following prepares the data for the forward propuugation. \n",
    "        \"\"\"\n",
    "        \"\"\"First, Grab the hardcoded values from the states\"\"\"\n",
    "        # -- [T x B x H x W]\n",
    "        glyphs = env_outputs[\"glyphs\"]\n",
    "        # -- [T x B x F]\n",
    "        blstats = env_outputs[\"blstats\"]\n",
    "        \n",
    "        \"\"\"Shape of the input layer, extract T and B\"\"\"\n",
    "        T, B, *_ = glyphs.shape\n",
    "        \n",
    "        \"\"\"Condense the tensor into a n sized vector\"\"\"\n",
    "        # -- [B' x H x W]\n",
    "        glyphs = torch.flatten(glyphs, 0, 1)  # Merge time and batch.\n",
    "        \n",
    "        \"\"\"Not sure what this code does?\"\"\"\n",
    "        # -- [B' x F]\n",
    "        blstats = blstats.view(T * B, -1).float()\n",
    "\n",
    "        # -- [B x H x W]\n",
    "        glyphs = glyphs.long()\n",
    "        # -- [B x 2] x,y coordinates\n",
    "        coordinates = blstats[:, :2]\n",
    "        # TODO ???\n",
    "        # coordinates[:, 0].add_(-1)\n",
    "\n",
    "        # -- [B x F]\n",
    "        blstats = blstats.view(T * B, -1).float()\n",
    "        # -- [B x K]\n",
    "        blstats_emb = self.embed_blstats(blstats)\n",
    "\n",
    "        assert blstats_emb.shape[0] == T * B\n",
    "\n",
    "        reps = [blstats_emb]\n",
    "\n",
    "        # -- [B x H' x W']\n",
    "        crop = self.crop(glyphs, coordinates)\n",
    "\n",
    "        # print(\"crop\", crop)\n",
    "        # print(\"at_xy\", glyphs[:, coordinates[:, 1].long(), coordinates[:, 0].long()])\n",
    "\n",
    "        # -- [B x H' x W' x K]\n",
    "        crop_emb = self._select(self.embed, crop)\n",
    "\n",
    "        # CNN crop model.\n",
    "        # -- [B x K x W' x H']\n",
    "        crop_emb = crop_emb.transpose(1, 3)  # -- TODO: slow?\n",
    "        # -- [B x W' x H' x K]\n",
    "        crop_rep = self.extract_crop_representation(crop_emb)\n",
    "\n",
    "        # -- [B x K']\n",
    "        crop_rep = crop_rep.view(T * B, -1)\n",
    "        assert crop_rep.shape[0] == T * B\n",
    "\n",
    "        reps.append(crop_rep)\n",
    "\n",
    "        # -- [B x H x W x K]\n",
    "        glyphs_emb = self._select(self.embed, glyphs)\n",
    "        # glyphs_emb = self.embed(glyphs)\n",
    "        # -- [B x K x W x H]\n",
    "        glyphs_emb = glyphs_emb.transpose(1, 3)  # -- TODO: slow?\n",
    "        # -- [B x W x H x K]\n",
    "        glyphs_rep = self.extract_representation(glyphs_emb)\n",
    "\n",
    "        # -- [B x K']\n",
    "        glyphs_rep = glyphs_rep.view(T * B, -1)\n",
    "\n",
    "        assert glyphs_rep.shape[0] == T * B\n",
    "\n",
    "        # -- [B x K'']\n",
    "        reps.append(glyphs_rep)\n",
    "\n",
    "        st = torch.cat(reps, dim=1)\n",
    "\n",
    "        # -- [B x K]\n",
    "        st = self.fc(st)\n",
    "        \n",
    "        \"\"\"\n",
    "        Use of an LSTM for forward prop? \n",
    "        \"\"\"\n",
    "        if self.use_lstm:\n",
    "            core_input = st.view(T, B, -1)\n",
    "            core_output_list = []\n",
    "            notdone = (~env_outputs[\"done\"]).float()\n",
    "            for input, nd in zip(core_input.unbind(), notdone.unbind()):\n",
    "                # Reset core state to zero whenever an episode ended.\n",
    "                # Make `done` broadcastable with (num_layers, B, hidden_size)\n",
    "                # states:\n",
    "                nd = nd.view(1, -1, 1)\n",
    "                core_state = tuple(nd * s for s in core_state)\n",
    "                output, core_state = self.core(input.unsqueeze(0), core_state)\n",
    "                core_output_list.append(output)\n",
    "            core_output = torch.flatten(torch.cat(core_output_list), 0, 1)\n",
    "        else:\n",
    "            core_output = st\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the policy values. These policy values will determine the action we should take. \n",
    "        \"\"\"\n",
    "        # -- [B x A]\n",
    "        policy_logits = self.policy(core_output)\n",
    "        # -- [B x A]\n",
    "        baseline = self.baseline(core_output)\n",
    "        \n",
    "        \"\"\"\n",
    "        If training, we want to sample those actions that are the highest probability. \n",
    "        Otherwise, we just want to take the maximum probability. \n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
    "        else:\n",
    "            # Don't sample when testing.\n",
    "            action = torch.argmax(policy_logits, dim=1)\n",
    "\n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        baseline = baseline.view(T, B)\n",
    "        action = action.view(T, B)\n",
    "\n",
    "        return (\n",
    "            dict(policy_logits=policy_logits, baseline=baseline, action=action),\n",
    "            core_state,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be71746-217b-41fc-a8c5-c17e4eb41e9c",
   "metadata": {},
   "source": [
    "# Declare a new NetHack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2829895f-1b98-4527-bf5c-4f815ae68f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a new NetHackNet class. Calls it Net. \n",
    "\"\"\" \n",
    "Net = NetHackNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92970717-4cb4-42f7-b42f-977017ece0c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aa1fc5f-a66e-44cf-b4f2-587537da83a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b2ace1e5def3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'flags' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train\n",
    "\"\"\"\n",
    "train(flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56aef81-22e8-495c-8fc3-c763c2f9bcc8",
   "metadata": {},
   "source": [
    "## Test Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a122aa0-8dca-4e29-9ca7-69b4f3bce093",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
